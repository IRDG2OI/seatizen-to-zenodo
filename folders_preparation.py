#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jul 10 15:04:46 2023

@author: g2oi
"""

from jacques.inference import predictor
import os
import pandas as pd
import shutil
import argparse
import time
from datetime import datetime
import glob
from predict import annotation_model
import json
import zipfile
from PIL import Image
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter, landscape
from reportlab.platypus import Table
from reportlab.lib import colors
import random
import pyreadr
import warnings
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
from cartopy.io.img_tiles import GoogleTiles
import numpy as np

warnings.simplefilter(action = "ignore", category = RuntimeWarning)

# get folder list that contains images (['DCIM/100GOPRO/', 'DCIM/101GOPRO/'])
def get_subfolders(folder_path):
    subfolders = []
    for item in os.listdir(folder_path):
        item_path = os.path.join(folder_path, item)
        if os.path.isdir(item_path) and 'GOPRO' in item:
            subfolder_name = f"DCIM/{os.path.basename(item_path)}"
            subfolders.append(subfolder_name)
    return subfolders

# List ONLY directories
def list_directories(path):
    directories = [entry for entry in os.listdir(path) if os.path.isdir(os.path.join(path, entry))]
    return directories

# Delete "BEFORE" and "AFTER" folders at specified path if they exists
def delete_folders(path):
    before_path = os.path.join(path, 'BEFORE')
    after_path = os.path.join(path, 'AFTER')

    # Check if "BEFORE" folder exists and delete it
    if os.path.exists(before_path) and os.path.isdir(before_path):
        shutil.rmtree(before_path)
        print(f'Deleted folder: {before_path}')

    # Check if "AFTER" folder exists and delete it
    if os.path.exists(after_path) and os.path.isdir(after_path):
        shutil.rmtree(after_path)
        print(f'Deleted folder: {after_path}')

# return the list of sessions
def get_sessions_list(sessions, session_index):
    if(isinstance(sessions, str)): # if it's a string, it's one directory
        # list sessions automatically from one directory
        directory_of_sessions = sessions
        list_of_sessions = list_directories(directory_of_sessions)
        list_of_sessions = [os.path.join(directory_of_sessions, session) for session in list_of_sessions]
        list_of_sessions.sort()
        if session_index != -1:
            list_of_sessions = [list_of_sessions[session_index]]
    elif(isinstance(sessions, list)): # if it's a list, sessions are written by hand
        if session_index != -1:
            list_of_sessions = [sessions[session_index]]
        else:
            list_of_sessions = sessions
    
    return list_of_sessions

def move_out_images(csv_file, destination_directory, who_moves=['useless', 'useful']):
    '''
    Function that moves images predicted as useless or useful in the destination path.
    # Input:
    - csv_file : the path to the classification csv generated by jacques
    - destination_directory : the path where the images will be moved
    - who_moves : whether to move the useful or useless images in another directory
    # Output:
    Images are moved into the destination path
    '''
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file)
    
    # Filter rows with the chosen class
    useless_images = df[df["class"] == who_moves]
    
    # Create destination directory if it doesn't exists
    os.makedirs(destination_directory, exist_ok=True)
    
    # Move images to another directory
    for _, row in useless_images.iterrows():
        image_filename = row["image"]
        source_directory = row["dir"]  # Extract source directory from the 'dir' column
        source_path = os.path.join(source_directory, image_filename)
        destination_path = os.path.join(destination_directory, image_filename)

        # Check if the image exists in the source directory before moving
        if os.path.exists(source_path):
            shutil.move(source_path, destination_path)
            print(f"Moved image '{image_filename}' to '{destination_directory}'.")
        else:
            print(f"Image '{image_filename}' not found in '{source_directory}'.")

# move back useless images to their original folders
def move_back_images(csv_path, useless_img_path):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_path)
    
    # Iterate over each row in the DataFrame
    for index, row in df.iterrows():
        image_path = useless_img_path + row['image']
        image_class = row['class']
        original_dir = row['dir']
    
        # Check if the image class is 'useless'
        if image_class == 'useless':
            # Create the original directory if it doesn't exist
            # os.makedirs(original_dir, exist_ok=True)
            # Move the image back to its original directory
            shutil.move(image_path, original_dir)
    
            # Display the operation for confirmation
            print(f"Moved image '{image_path}' back to '{original_dir}'.")

# merge multilabel annotations csv with GPS metadata (latitude, longitude and date)
def join_GPS_metadata(annotation_csv_path, gps_info_csv_path, merged_csv_path):
    annot_df = pd.read_csv(annotation_csv_path)
    gps_df = pd.read_csv(gps_info_csv_path)
    
    # Extract image names from the file paths
#     annot_df['Image_name'] = annot_df['Image_path'].str.split('/').str[-1]
    annot_df['Image_name'] = annot_df['image']
    try:
        gps_df['Image_name'] = gps_df['photo_relative_file_path'].str.split('/').str[-1]
    except KeyError:
        gps_df['Image_name'] = gps_df['FileName']
    
    # Merge the DataFrames based on the image names
    try:
        merged_df = annot_df.merge(gps_df[['Image_name', 'decimalLatitude', 'decimalLongitude', 'GPSDateTime']],
                                on='Image_name', how='left')
    except KeyError:
        merged_df = annot_df.merge(gps_df[['Image_name', 'Composite:GPSLatitude', 'Composite:GPSLongitude', 'SubSecDateTimeOriginal']],
                                on='Image_name', how='left')
    
    # Drop the 'Image_name' column from merged_df
    merged_df.drop(columns='Image_name', inplace=True)
    
    # Swapping latitude and longitude
    try:
        merged_df = merged_df.rename(columns = {'decimalLatitude':'decimalLongitude', 'decimalLongitude':'decimalLatitude'})
    except KeyError:
        pass
    
    merged_df.to_csv(merged_csv_path, index=False, header=True)
    
# get a list of merged files from a given path
def get_merged_files(path):
    merged_files = []
    
    # Get a list of all files and directories in the given path
    files_and_dirs = os.listdir(path)
    
    # Filter out the files that start with 'merged'
    for file in files_and_dirs:
        if file.startswith('merged') and os.path.isfile(os.path.join(path, file)):
            merged_files.append(file)
    
    return merged_files

# apply probabilities thresholds to the values of multilabel annotations
def apply_thresholds(merged_csv_path, thresholds_csv_path, final_csv_path):
    df_1 = pd.read_csv(merged_csv_path)
    df_2 = pd.read_csv(thresholds_csv_path)
    
    for column in df_1.columns:
        if column in df_2.columns:
            threshold_value = df_2[column][0]
            df_1[column] = df_1[column].apply(lambda x: x if x > threshold_value else 0)
            
    df_1.to_csv(final_csv_path, index=False, header=True)
    
# filter out useless images in the final csv based on jacques classification csv
def filter_useless_images(classification_csv, final_csv_path):
    df_1 = pd.read_csv(classification_csv)
    df_2 = pd.read_csv(final_csv_path)
    
    # Extract image name from image path
    df_2['image'] = df_2['Image_path'].str.split('/').str[-1]
    
    # Filter out the 'useless' images from the df_1
    df1_useful = df_1[df_1['class'] == 'useful']
    
    # Merge the two DF on the 'image' column and keep only rows that exist in both DF
    final_df = pd.merge(df_2, df1_useful, on='image', how='inner')
    
    # Removing useless columns
    final_df = final_df.drop(columns=['dir', 'class', 'image'])
    
    final_csv_path = final_csv_path[:-4] + '_filtered' + final_csv_path[-4:]
    
    final_df.to_csv(final_csv_path, index=False, header=True)

# merge all final csv files starting with 'final' located at csv_path in one csv file
def merge_all_final_csv(csv_path):
    directory_path = os.path.dirname(csv_path)
    wildcard_pattern = os.path.join(directory_path, 'final_*.csv')
    file_list = glob.glob(wildcard_pattern)
    
    dfs = []
    
    for file in file_list:
        df = pd.read_csv(file)
        dfs.append(df)
    
    try:
        merged_df = pd.concat(dfs, ignore_index=True)
    except Exception: # if there is only one final_ csv file
        merged_df = df
    
    merged_df["sessions"] = merged_df["dir"].apply(lambda x: os.path.basename(os.path.dirname(os.path.dirname(x))))
    merged_df.sort_values(by='sessions', inplace=True)
    merged_df = merged_df.drop(columns='sessions')
    
    merged_csv_path = os.path.join(directory_path, 'all_sessions_annotation.csv')
    
    merged_df.to_csv(merged_csv_path, index=False, header=True)

def unzip(zip_file_index, source_folder, destination_folder):
    zip_files = [file for file in os.listdir(source_folder) if file.endswith('.zip')]
    zip_files.sort()
    zip_file = zip_files[zip_file_index]
    zip_file_path = os.path.join(source_folder, zip_file)
    session_name = zip_file[:-4]
    destination_folder = os.path.join(destination_folder, session_name)
    os.makedirs(destination_folder, exist_ok=True)
    
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(destination_folder)
        
def zip_folders(session, session_name, destination_folder):
    zip_filename = session_name + ".zip"
    zip_path = os.path.join(destination_folder, zip_filename)
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Walk through the folder and add each file and subfolder to the ZIP file
        for root, dirs, files in os.walk(session):
            for file in files:
                file_path = os.path.join(root, file)
                # The second argument of 'zipf.write' is the relative path inside the ZIP
                zipf.write(file_path, os.path.relpath(file_path, session))

    print(f"{session_name} zipped in {destination_folder} successfully.\n")

def copy_and_zip_folder(src_folder, dest_folder, session_name):
    '''
    Function to temporarily copy the session folder and zip every folders in it plus subfolders of PROCESSED_DATA.
    You will get as output a folder with .zip archives in it.
    '''
    session_folder = os.path.join(dest_folder, session_name)

    # Copy the source folder to the destination folder
    shutil.copytree(src_folder, session_folder)

    # List subdirectories in the session folder
    subdirs = [os.path.join(session_folder, subdir) for subdir in os.listdir(session_folder) if os.path.isdir(os.path.join(session_folder, subdir))]

    # Zip subfolders:
    for subdir in subdirs:
        subdir_name = os.path.basename(subdir)

        if subdir_name in ["DCIM", "DCIM_THUMBNAILS", "GPS", "METADATA", "PROCESSED_DATA", "SENSORS"]:
            if subdir_name == "PROCESSED_DATA": # zipping folders in PROCESSED_DATA
                subsubdirs = [os.path.join(subdir, subsubdir) for subsubdir in os.listdir(subdir) if os.path.isdir(os.path.join(subdir, subsubdir))]
                for subsubdir in subsubdirs:
                    subsubdir_name = os.path.basename(subsubdir)
                    zip_filename = os.path.join(subdir, f"{subsubdir_name}.zip")
                    with zipfile.ZipFile(zip_filename, "w", zipfile.ZIP_DEFLATED) as zipf:
                        for root, dirs, files in os.walk(subsubdir):
                            for file in files:
                                file_path = os.path.join(root, file)
                                arcname = os.path.relpath(file_path, subsubdir)
                                zipf.write(file_path, arcname)

                    shutil.rmtree(subsubdir)

            # zipping every subdir
            zip_filename = os.path.join(session_folder, f"{subdir_name}.zip")
            with zipfile.ZipFile(zip_filename, "w", zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(subdir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, subdir)
                        zipf.write(file_path, arcname)

            # Delete the original subdirectory and its contents
            shutil.rmtree(subdir)

def create_sessions_stats(session, session_name, jacques_model_path):
    '''
    Function that creates a .txt file with jacques classification statistics 
    (total images, number of useful/useless images, percentage of useless images in relation of total images)
    '''
    jacques_model = jacques_model_path.split("/")[-1]
    jacques_model = jacques_model.split(".")[0]
    input_file = os.path.join(session, f'LABEL/classification_{session_name}_jacques-v0.1.0_model-{jacques_model}.csv')
    output_file = os.path.join(session, f'METADATA/{session_name}_stats.txt')

    if not os.path.exists(os.path.dirname(input_file)):
        input_file = os.path.join(session, f'PROCESSED_DATA/IA/classification_{session_name}_jacques-v0.1.0_model-{jacques_model}.csv')
    
    df = pd.read_csv(input_file)
    total_images = len(df)
    useful_images = len(df[df['class'] == 'useful'])
    useless_images = len(df[df['class'] == 'useless'])
    percentage_useless = (useless_images / total_images) * 100
    
    with open(output_file, 'w') as f:
        f.write(f"Total images: {total_images}\n")
        f.write(f"Useful images: {useful_images}\n")
        f.write(f"Useless images: {useless_images}\n")
        f.write(f"Percentage of useless images: {percentage_useless:.2f}%\n")
    
    print(f"{session_name} statistics written to {output_file}.")

def resize_images(source_folder, destination_folder, file_name):
    '''
    Function to resize images to thumbnail size.
    '''
    destination_path = os.path.join(destination_folder, file_name)
    image_path = os.path.join(source_folder, file_name)
    image = Image.open(image_path)
    # Resize the images where shortest side is 256 pixels, keeping aspect ratio. 
    if image.width > image.height: 
        factor = image.width/image.height
        image = image.resize(size=(int(round(factor*256,0)),256))
    else:
        factor = image.height/image.width
        image = image.resize(size=(256, int(round(factor*256,0))))
    # Crop out the center 224x224 portion of the image.

    image = image.crop(box=((image.width/2)-112, (image.height/2)-112, (image.width/2)+112, (image.height/2)+112))

    image.save(destination_path)

def create_thumbnails_for_FRAMES(session):
    '''
    Create thumbnails for images located in the folder /PROCESSED_DATA/FRAMES/
    '''
    images_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
    file_list = os.listdir(images_folder_path)
    destination_folder = os.path.join(session, 'DCIM_THUMBNAILS/')
    os.makedirs(destination_folder, exist_ok=True)
    for file_name in file_list:
        try:
            resize_images(images_folder_path, destination_folder, file_name)
        except Exception as e:
            print(f"[ERROR] Failed to create thumbnail of {file_name}: {e}")


def create_thumbnails(session):
    '''
    This function creates a folder THUMBNAILS and save in it resized images of the processed session.
    '''
    images_folder_path = f'{session}/DCIM/'
    if os.path.exists(images_folder_path):
        list_of_dir = get_subfolders(f'{session}/DCIM/')
        if len(list_of_dir) != 0:
            for directory in list_of_dir:
                directory_name = directory.split("/")[-1]
                print(f"Creating thumbnails for directory {directory_name}")
                destination_folder = os.path.join(session, "DCIM_THUMBNAILS/")
                os.makedirs(destination_folder, exist_ok=True)
                folder_path = os.path.join(session, directory)
                file_list = os.listdir(folder_path)
                for file_name in file_list:
                    try:
                        resize_images(folder_path, destination_folder, file_name)
                    except Exception as e:
                        print(f"[ERROR] Failed to create thumbnail of {file_name}: {e}")
        else:
            create_thumbnails_for_FRAMES(session)        
    else:
        create_thumbnails_for_FRAMES(session)
        

def process_frames(session, results_of_all_sessions, directory, jacques_model_path):
    '''
    Function to launch jacques classification on images located in the folder /PROCESSED_DATA/FRAMES/
    '''
    print("\n[Frames processing]")
    folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
    file_list = os.listdir(folder_path)

    for file_name in file_list:
        if file_name.startswith('._'):
            file_path = os.path.join(folder_path, file_name)
            try:
                os.remove(file_path)
                print(f"\nRemoved unidentified image: {file_name}")
            except OSError as e:
                print(f"\nError removing image {file_name}: {e}")

    try:
        results = predictor.classify_useless_images(folder_path=folder_path, ckpt_path=jacques_model_path)
        results_of_all_sessions = pd.concat([results_of_all_sessions, results], axis=0, ignore_index=True)
        return results_of_all_sessions
    except Exception as e:
        print(f"\n[ERROR] Classification error in {directory}: {e}")
        pass

def evenly_select_images_on_interval(image_list):
    total_images = len(image_list)
    index_list = np.linspace(0, total_images, 100, dtype=int, endpoint=False)
    selected_images = [image_list[i] for i in index_list]
    return selected_images

def create_trajectory_map(metadata_path):
    df = pd.read_csv(metadata_path)
        
    fig = plt.figure(figsize=(2,2), dpi=300)
    ax = fig.add_subplot(projection=ccrs.PlateCarree())
    ax.set_extent([df.GPSLongitude.min()-0.005, df.GPSLongitude.max()+0.005, df.GPSLatitude.min()-0.001,df.GPSLatitude.max()+0.001])

    imagery = GoogleTiles(url='https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}')
    ax.add_image(imagery, 19)

    ax.plot(df.GPSLongitude, df.GPSLatitude, color='yellow', linewidth=1)
    fig.savefig("map.png", bbox_inches='tight',pad_inches=0, dpi=300)
    print("Trajectory map created!")

def create_pdf_preview(pdf_preview_path, session, session_name, list_of_images):
    '''
    Function to create a pdf preview of the session. It will contains:
    - a trajectory map
    - 10 thumbnails of images selected randomly in the session
    - a sneakpeek to the metadata file of the session
    '''

    # PDF creation
    pdf_file = os.path.join(pdf_preview_path, f"000_{session_name}_preview.pdf")
    c = canvas.Canvas(pdf_file, pagesize=letter)
    page_width, page_height = letter
    max_height = page_height - 100

    c.setFont("Helvetica-Bold", 14)
    c.drawString(30, 730, "Session Summary")
    c.setFont("Helvetica-Bold", 18)
    c.setFillColor(colors.blue)
    c.drawString(30, 705, session_name)

    # Trajectory map
    metadata_path = os.path.join(session, "METADATA/metadata.csv")

    if not os.path.exists(metadata_path):
        # metadata_path = os.path.join(session, f"METADATA/exif/All_Exif_metadata_{session_name}.RDS")
        print("\nMetadata file not found for trajectory map creation!")

    create_trajectory_map(metadata_path)
    print("Adding map to the PDF...")
    c.drawImage("map.png", 20, 300)
    os.remove("map.png") # deleting map.png

    c.setFont("Helvetica-Bold", 16)
    c.setFillColor(colors.black)
    c.drawString(30, 650, "Trajectory map")
    print("Map added!")
    # Thumbnails
    c.showPage()
    c.setFont("Helvetica-Bold", 16)
    c.drawString(30, 730, "Images previews")

    selected_images = evenly_select_images_on_interval(list_of_images)
    print("Images previews selected!\n")
    x_coord = 30
    y_coord = max_height

    for i, image in enumerate(selected_images):
        if i % 5 == 0 and i != 0:
            # Start a new row of images
            x_coord = 30
            y_coord -= 110

        img = Image.open(image)
        img.thumbnail((100, 100))

        img_width, img_height = img.size

        temp_image_path = os.path.join(pdf_preview_path, f'temp_{i}.jpg')
        img.save(temp_image_path)

        if y_coord - img_height < 50:
            c.showPage()
            y_coord = max_height

        c.drawImage(temp_image_path, x_coord, y_coord - img_height)

        os.remove(temp_image_path)

        x_coord += 110

    c.showPage()
    c.setPageSize(landscape(letter))

    # Metadata preview
    print("Loading data for metadata preview...")
    metadata_file = os.path.join(session, f"METADATA/metadata.csv")
    df = pd.read_csv(metadata_file)
    print("Data loaded!")
    preview_df = df.head(20)
    print("Preview dataframe created!")
    first_columns = preview_df.iloc[:,:6]
    print("Creation of the PDF table...")
    table_data = [list(first_columns.columns)] + first_columns.values.tolist()
    table = Table(table_data)

    table.setStyle([
        ('TEXTCOLOR', (0, 0), (-1, 0), (0, 0, 1)),  # Header row text color (blue)
        ('FONTSIZE', (0, 1), (-1, -1), 8), # Font size of all cells
        ('BACKGROUND', (0, 0), (-1, 0), (0.7, 0.7, 0.7)),  # Header row background color (gray)
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),  # Center align all cells
        ('INNERGRID', (0, 0), (-1, -1), 0.25, (0, 0, 0)),  # Inner gridlines
        ('BOX', (0, 0), (-1, -1), 0.25, (0, 0, 0)),  # Cell borders
    ])

    table.wrapOn(c, 10, 20)
    table.drawOn(c, 30, 100)
    print("PDF table sucessfully created!")
    c.setFont("Helvetica-Bold", 16)
    c.drawString(30, 530, "Metadata preview")
    
    c.save()
    print("PDF created!")

def create_metadata_image_csv(sessions, global_data_path):
    list_of_sessions = get_sessions_list(sessions, -1)
    dfs = []
    newFileNameList = []

    for session in list_of_sessions:
        session_name = session.split('/')[-1]
        session_metadata_path = os.path.join(session, "METADATA/metadata.csv")
        if os.path.exists(session_metadata_path):
            df = pd.read_csv(session_metadata_path)
            if not df["FileName"][0].startswith(session_name): # checking if original filename is not already in the correct format
                newFileNameList.append([session_name +"_"+ filename for filename in df["FileName"]])
            else:
                newFileNameList.append(df["FileName"].tolist())
            dfs.append(df)
        else:
            print("\n[ERROR] Metadata file not found!")
            # session_metadata_path = os.path.join(session, "METADATA/metadata_enriched.RDS")
            # rds_data = pyreadr.read_r(session_metadata_path)
            # df = list(rds_data.values())[0]
            # dfs.append(df)


    if len(dfs) != 0:
        try:
            merged_df = pd.concat(dfs, ignore_index=True)
        except Exception: # if there is only one session
            merged_df = df

        merged_df = merged_df.rename(columns={"FileName":"OriginalFileName"})

        fileNameList = [item for sublist in newFileNameList for item in sublist]
        merged_df["FileName"] = fileNameList

        cols_to_keep = ["OriginalFileName", "FileName", "GPSLatitude", "GPSLongitude", "ApertureValue", "Compression", "Contrast", "CreateDate", "DateCreated", "DateTimeDigitized", "DateTimeOriginal", 
                        "DigitalZoomRatio", "ExifImageHeight", "ExifImageWidth", "ExifToolVersion", "ExifVersion", "ExposureCompensation", "ExposureMode", "ExposureProgram", "FileSize", "FileType",  
                        "FileTypeExtension", "FNumber", "FocalLength", "FocalLength35efl", "FocalLengthIn35mmFormat", "FOV", "GPSAltitude", "GPSAltitudeRef", "GPSDateTime", "GPSDate", "GPSTime", "GPSLatitudeRef", 
                        "GPSLongitudeRef", "GPSMapDatum", "GPSPosition", "GPSTimeStamp", "GPSRoll", "GPSPitch", "GPSTrack", "ImageHeight", "ImageWidth", "LightValue", "Make", "MaxApertureValue", 
                        "MaximumShutterAngle", "Megapixels", "MeteringMode", "MIMEType", "Model", "Saturation", "ScaleFactor35efl", "SceneCaptureType", "SceneType", "SensingMethod", "Sharpness", 
                        "ShutterSpeed", "Software", "SubSecDateTimeOriginal", "ThumbnailImage", "ThumbnailLength", "ThumbnailOffset", "WhiteBalance", "XResolution", "YResolution","GPSfix", "GPSsdne", "GPSsde", "GPSsdn"]
        
        filtered_columns = [col for col in cols_to_keep if col in merged_df.columns] # only keep columns that exists in merged_df

        merged_df = merged_df[filtered_columns]

        merged_csv_path = os.path.join(global_data_path, 'metadata_image.csv')
        merged_df.to_csv(merged_csv_path, index=False, header=True)
        print("metadata_image.csv created!")
    

def classify_sessions(sessions, session_index, jacques_model_path):
    '''
    Function that uses jacques predicator classify_useless_images function to classify given sessions images.
    # Input:
        - sessions: 
            ## can be a string that refer to a single directory that contains 
            every sessions (ex: 'sessions/')
            ## can be a list of the desired sessions (ex: ['sessions/session_2017_11_05_kite_Le_Morne'])
        - session_index:
            ## index of the session to process
        - jacques_model_path:
            ## path to jacques model
    # Output:
        - a dataframe that contains the classification result for the selected sessions images
        
    '''
    list_of_sessions = get_sessions_list(sessions, session_index)
    
    # classification of useful and useless images of all sessions
    results_of_all_sessions = pd.DataFrame(columns = ['dir', 'image', 'class'])
    for session in list_of_sessions:
        print("-----------------------------------------------")
        print(session)
        print("-----------------------------------------------")
        dcim_path = f'{session}/DCIM/'
        if os.path.exists(dcim_path):
            list_of_dir = get_subfolders(dcim_path)
            if len(list_of_dir) != 0:
                print("\n[Images processing]")
                for directory in list_of_dir:
                    print('\n' + directory)
                    folder_path = os.path.join(session, directory)
                    file_list = os.listdir(folder_path)
                    for file_name in file_list:
                        if file_name.startswith('._'):
                            file_path = os.path.join(folder_path, file_name)
                            try:
                                os.remove(file_path)
                                print(f"\nRemoved unidentified image: {file_name}")
                            except OSError as e:
                                print(f"\nError removing image {file_name}: {e}")

                    try:
                        results = predictor.classify_useless_images(folder_path=folder_path, ckpt_path=jacques_model_path)
                        results_of_all_sessions = pd.concat([results_of_all_sessions, results], axis=0, ignore_index=True)
                    except Exception as e:
                        print(f"\n[ERROR] Classification error in {directory}: {e}")
                        pass
            else: # if there are no folders in DCIM
                dcim_path = f'{session}/PROCESSED_DATA/FRAMES/'
                results_of_all_sessions = process_frames(session, results_of_all_sessions, dcim_path, jacques_model_path)
        else: # if DCIM doesn't exists, it means we are dealing with frames
            dcim_path = f'{session}/PROCESSED_DATA/FRAMES/'
            results_of_all_sessions = process_frames(session, results_of_all_sessions, dcim_path, jacques_model_path)
        
                    

    return results_of_all_sessions

def restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, pdf_preview_path, global_data_path, delete_before_after):
    '''
    Function to restructure sessions folders to be "Zenodo ready"
    # Input:
        - sessions:
            ## the list of sessions:
                either a single directory that contains every sessions. (ex: '/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_unzipped/')
                or a list of sessions path (ex: ['/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_unzipped/session_2017_11_19_paddle_Black_Rocks'])
        - zip_sessions:
            ## either to zip the sessions or not
        - dest_path:
            ## destination path where useless images will be moved. (ex: '/home3/datawork/aboyer/mauritiusSessionsOutput/useless_images/')
        - annot_path:
            ## path where multilabel annotations csv will be created. (ex: '/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_processing_output/results_csv/herbier_classification/herbier_classification_.csv')
        - jacques_model_path:
            ## path to jacques model. (ex: '/home/datawork-iot-nos/Seatizen/models/useless_classification/version_17/checkpoints/epoch=7-step=2056.ckpt')
        - annotation_model_path:
            ## path to annotation model. (ex: '/home/datawork-iot-nos/Seatizen/mauritius_use_case/Mauritius/models/multilabel_with_sable.pth')
        - thresholds_labels:
            ## a dictionnary that contains labels names and their associated thresholds
        - pdf_preview_path:
            ## path where a pdf preview will be created for each session
        - global_data_path:
            ## path where metadata_image.csv will be created
        - delete_before_after:
            ## boolean value to indicate if the user want to delete before and after folders if they exists
    # Output:
        - sessions folders "Zenodo ready"
    '''
    if len(sessions) != 0:
        # classification
        if len(jacques_model_path) != 0:
            results_of_all_sessions = classify_sessions(sessions, session_index, jacques_model_path)
        else:
            results_of_all_sessions = pd.DataFrame(data={}) # empty dataframe
        
        list_of_sessions = get_sessions_list(sessions, session_index)
        
        # operations on each session
        for session in list_of_sessions:
            session_name = session.split('/')[-1]

            if not results_of_all_sessions.empty:
                # export results to csv file
                model = jacques_model_path.split('/')[-1]
                # adding session name, jacques version and classification model version to csv filename
                suffix = f"{session_name}_jacques-v0.1.0_model-{model.split('.')[0]}"
                class_path = os.path.join(session, 'LABEL/classification_.csv')

                if not os.path.exists(os.path.dirname(class_path)):
                    class_path = os.path.join(session, 'PROCESSED_DATA/IA/')
                    os.makedirs(class_path, exist_ok=True)
                    class_path = os.path.join(class_path, 'classification_.csv')

                class_path = class_path[:-4] + suffix + class_path[-4:]
                results_of_all_sessions.to_csv(class_path, index = False, header = True)
                print(f'\nClassification informations written at {class_path}\n')

                # create a .txt file in the METADATA folder of each session. This file give statistics about the jacques classification result.
                try:
                    print(f"Creation of {session_name} jacques stats file...")
                    create_sessions_stats(session, session_name, jacques_model_path)
                except Exception as e:
                    print(e)
            
            # move useless images
            if len(dest_path) != 0:
                dest_path = os.path.join(dest_path, session_name)
                move_out_images(class_path, dest_path, who_moves='useless')
            
            image_folder_path = f'{session}/DCIM/'
            if os.path.exists(image_folder_path):
                # checking if there are "GOPRO" folders in DCIM
                list_of_dir = get_subfolders(image_folder_path)
                if len(list_of_dir) == 0:
                    image_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
                else:
                    # delete BEFORE and AFTER folders if they exists
                    if delete_before_after:
                        delete_folders(image_folder_path) 
            else:
                image_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
            
            for key, path in annot_path.items():
                if len(path) != 0:
                    print(f"\nStarting annotations using {key} model.")
                    csv_annotation_path = path[:-4] + session_name + path[-4:]
                    final_csv_path = os.path.join(os.path.dirname(csv_annotation_path), "final_annotation_.csv")
                    final_csv_path = final_csv_path[:-4] + session_name + final_csv_path[-4:]

                    try:
                        model_path = annotation_model_path.get(key)
                    except KeyError:
                        print(f"{key} not found in annotation_model_path definition in config.json file.")
                    try:
                        model_thresholds_labels = threshold_labels.get(key)
                    except KeyError:
                        print(f"{key} not found in threshold_labels definition in config.json file.")

                    # adding model annotations
                    
                    list_of_dir = get_subfolders(image_folder_path)
                    list_of_df = []

                    if len(list_of_dir) != 0:
                        for directory in list_of_dir:
                            print(f"\nAnnotations of images located in {directory}...")
                            images_path = os.path.join(session, directory)
                            df = annotation_model(images_path, csv_annotation_path, model_path, model_thresholds_labels)
                            list_of_df.append(df)

                        df = pd.concat(list_of_df, ignore_index=True)
                    else:
                        df = annotation_model(image_folder_path, csv_annotation_path, model_path, model_thresholds_labels)
                        
                    df.sort_values(by="image", inplace=True)
                    df.to_csv(csv_annotation_path, index = False, header = True)
                    print(f'\nAnnotations CSV of {session} successfully created at {csv_annotation_path}\n')
                    
                    # join GPS metadata to annotation file
                    gps_info_csv_path = f'{session}/GPS/photos_location_{session_name}.csv'
                    if os.path.exists(gps_info_csv_path):
                        join_GPS_metadata(csv_annotation_path, gps_info_csv_path, final_csv_path)
                        print(f'Merged GPS metadata with annotation results at {final_csv_path}\n')
                    else:
                        gps_info_csv_path = f'{session}/METADATA/metadata.csv'
                        join_GPS_metadata(csv_annotation_path, gps_info_csv_path, final_csv_path)
                    
                    # merge all sessions final annotation
                    merge_all_final_csv(csv_annotation_path)

            # zip session folder if path is not empty in config.json
            if len(zipped_sessions_path) != 0:
                print(f"Zipping {session_name}...")
                try:
                    create_thumbnails(session)
                    copy_and_zip_folder(session, zipped_sessions_path, session_name)
                    print(f"\n{session_name} successfully zipped.")
                except Exception as e:
                    print(e)

            if len(pdf_preview_path) != 0:
                print(f"\nCreation of pdf preview for session {session_name}...")
                list_of_dir = get_subfolders(image_folder_path)
                list_of_images = []
                if len(list_of_dir) != 0:
                    for directory in list_of_dir:
                        images_path = os.path.join(session, directory)
                        images_files = [f for f in os.listdir(images_path) if f.endswith('.JPG') or f.endswith('.jpg') or f.endswith('.jpeg')]
                        images_files.sort()
                        selected_images = [os.path.join(f'{images_path}/', image_name) for image_name in images_files]
                        list_of_images.append(selected_images)
                    list_of_images = [item for sublist in list_of_images for item in sublist]
                    create_pdf_preview(pdf_preview_path, session, session_name, list_of_images)
                else:
                    list_of_images = [f for f in os.listdir(image_folder_path) if f.endswith('.JPG') or f.endswith('.jpg') or f.endswith('.jpeg')]
                    list_of_images.sort()
                    list_of_images = [os.path.join(image_folder_path, image_name) for image_name in list_of_images]
                    create_pdf_preview(pdf_preview_path, session, session_name, list_of_images)

            if len(global_data_path) != 0:
                print("\nCreation of metadata_image.csv from all metadata files...")
                create_metadata_image_csv(sessions, global_data_path)


    else:
        print("[ERROR] You must fill in a path to your sessions in the config.json file! Refer to the README.md file for more informations.")
            
        
def main():
    start_time = time.time()
    print(f"Start time: {datetime.now()}")
    # getting session index from script args
    parser = argparse.ArgumentParser()
    parser.add_argument("--session-index",
                        action="store",
                        type=int,
                        default=argparse.SUPPRESS,
                        help="Index of the session that is being processed.")
    args = parser.parse_args()
    
    # read the config.json file
    with open('config.json') as json_file:
        config = json.load(json_file)
    
    # getting all variables from config.json file
    ## models
    jacques_model_path = config["paths"]["jacques_model_path"]
    annotation_model_path = config["paths"]["annotation_model_path"]
    ## paths
    sessions = config["paths"]["sessions_path"]
    zipped_sessions_path = config["paths"]["zipped_sessions_path"]
    pdf_preview_path = config["paths"]["pdf_preview_path"]
    global_data_path = config["paths"]["global_data_path"]
    dest_path = config["paths"]["useless_images_path"]
    annot_path = config["paths"]["annotation_csv_path"]
    ## threshold labels dictionnary
    threshold_labels = config["threshold_labels"]
    ## booleans
    delete_before_after = config["delete_before_after"]

    if hasattr(args, "session_index"):
        session_index = args.session_index - 1
        restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, pdf_preview_path, global_data_path, delete_before_after)
        execution_time_seconds = time.time() - start_time
        execution_time_minutes = "{:.2f}".format(execution_time_seconds / 60)
        print("\n========================================================")
        print(f"\nEnd time: {datetime.now()}")
        print(f"Total execution time: {execution_time_minutes} minutes")
    else: # execution on all sessions in the folder
        session_index = -1
        restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, pdf_preview_path, global_data_path, delete_before_after)
        execution_time_seconds = time.time() - start_time
        execution_time_minutes = "{:.2f}".format(execution_time_seconds / 60)
        print("\n========================================================")
        print(f"\nEnd time: {datetime.now()}")
        print(f"Total execution time: {execution_time_minutes} minutes")

if __name__ == '__main__':
    main()
