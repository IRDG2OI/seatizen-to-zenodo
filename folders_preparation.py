#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jul 10 15:04:46 2023

@author: g2oi
"""

from jacques.inference import predictor
import os
import pandas as pd
import shutil
import argparse
import time
from datetime import datetime
import glob
from predict import annotation_model
import json
import zipfile
from PIL import Image
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter, landscape
from reportlab.platypus import Table
from reportlab.lib import colors
import warnings
import matplotlib.pyplot as plt
import cartopy.crs as ccrs
from cartopy.io.img_tiles import GoogleTiles
import numpy as np

warnings.simplefilter(action = "ignore", category = RuntimeWarning)

"""
This script defines multiple functions that are used in the restructure_sessions() function.
This function is then called in main() at runtime.
"""

def get_subfolders(folder_path):
    '''
    Function to get the folder list that have 'GOPRO' in their names (['DCIM/100GOPRO/', 'DCIM/101GOPRO/'])
    '''
    subfolders = []
    for item in os.listdir(folder_path):
        item_path = os.path.join(folder_path, item)
        if os.path.isdir(item_path) and 'GOPRO' in item:
            subfolder_name = f"DCIM/{os.path.basename(item_path)}"
            subfolders.append(subfolder_name)
    return subfolders

def list_directories(path):
    '''
    Function that only list the directories located at the specified path.
    '''
    directories = [entry for entry in os.listdir(path) if os.path.isdir(os.path.join(path, entry))]
    return directories

def delete_folders(path):
    '''
    Function to delete "BEFORE", "AFTER" and "USELESS" folders at specified path if they exists.
    '''
    before_path = os.path.join(path, 'BEFORE')
    after_path = os.path.join(path, 'AFTER')
    useless_path = os.path.join(path, 'USELESS')

    # Check if "BEFORE" folder exists and delete it
    if os.path.exists(before_path) and os.path.isdir(before_path):
        shutil.rmtree(before_path)
        print(f'Deleted folder: {before_path}')

    # Check if "AFTER" folder exists and delete it
    if os.path.exists(after_path) and os.path.isdir(after_path):
        shutil.rmtree(after_path)
        print(f'Deleted folder: {after_path}')

    # Check if "USELESS" folder exists and delete it
    if os.path.exists(useless_path) and os.path.isdir(useless_path):
        shutil.rmtree(useless_path)
        print(f'Deleted folder: {useless_path}')

def get_sessions_list(sessions, session_index):
    '''
    Function that returns the list of sessions.
    - sessions can be a path to a directory or a list of directories.
    '''
    if(isinstance(sessions, str)): # if it's a string, it's one directory
        # list sessions automatically from one directory
        directory_of_sessions = sessions
        list_of_sessions = list_directories(directory_of_sessions)
        list_of_sessions = [os.path.join(directory_of_sessions, session) for session in list_of_sessions]
        list_of_sessions.sort()
        if session_index != -1:
            list_of_sessions = [list_of_sessions[session_index]]
    elif(isinstance(sessions, list)): # if it's a list, sessions are written by hand
        if session_index != -1:
            list_of_sessions = [sessions[session_index]]
        else:
            list_of_sessions = sessions
    
    return list_of_sessions

def move_out_images(csv_file, destination_directory, who_moves=['useless', 'useful']):
    '''
    Function that moves images predicted as useless or useful in the destination path.
    ### Input:
    - csv_file : the path to the classification csv generated by jacques
    - destination_directory : the path where the images will be moved
    - who_moves : whether to move the useful or useless images in another directory
    ### Output:
    Images are moved into the destination path
    '''
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file)
    
    # Filter rows with the chosen class
    useless_images = df[df["class"] == who_moves]
    
    # Create destination directory if it doesn't exists
    os.makedirs(destination_directory, exist_ok=True)
    
    # Move images to another directory
    for _, row in useless_images.iterrows():
        image_filename = row["image"]
        source_directory = row["dir"]  # Extract source directory from the 'dir' column
        source_path = os.path.join(source_directory, image_filename)
        destination_path = os.path.join(destination_directory, image_filename)

        # Check if the image exists in the source directory before moving
        if os.path.exists(source_path):
            shutil.move(source_path, destination_path)
            # print(f"Moved image '{image_filename}' to '{destination_directory}'.")
        else:
            print(f"Image '{image_filename}' not found in '{source_directory}'.")

def move_back_images(csv_path, useless_img_path):
    '''
    Function to move back useless images to their original folders
    '''
    df = pd.read_csv(csv_path)
    
    # Iterate over each row in the DataFrame
    for index, row in df.iterrows():
        image_path = useless_img_path + row['image']
        image_class = row['class']
        original_dir = row['dir']
    
        # Check if the image class is 'useless'
        if image_class == 'useless':
            # Move the image back to its original directory
            shutil.move(image_path, original_dir)
    
            # Display the operation for confirmation
            print(f"Moved image '{image_path}' back to '{original_dir}'.")

def join_GPS_metadata(annotation_csv_path, metadata_path, merged_csv_path):
    '''
    Function to merge multilabel annotations csv with GPS metadata (latitude, longitude and date)
    '''
    annot_df = pd.read_csv(annotation_csv_path)
    gps_df = pd.read_csv(metadata_path)
    
    # Extract image names from the file paths
    annot_df['Image_name'] = annot_df['image']
    gps_df['Image_name'] = gps_df['FileName']
    
    # Merge the DataFrames based on the image names
    try:
        merged_df = annot_df.merge(gps_df[['Image_name', 'GPSLatitude', 'GPSLongitude', 'GPSDateTime']],
                                on='Image_name', how='left')
    except KeyError:
        merged_df = annot_df.merge(gps_df[['Image_name', 'GPSLatitude', 'GPSLongitude', 'SubSecDateTimeOriginal']],
                                on='Image_name', how='left')
    
    # Drop the 'Image_name' column from merged_df
    merged_df.drop(columns='Image_name', inplace=True)
    
    merged_df.to_csv(merged_csv_path, index=False, header=True)

def merge_all_final_csv(csv_path):
    '''
    Function to merge all final csv files starting with 'final' located at csv_path in one csv file
    '''
    directory_path = os.path.dirname(csv_path)
    wildcard_pattern = os.path.join(directory_path, 'final_*.csv')
    file_list = glob.glob(wildcard_pattern)
    
    dfs = []
    
    for file in file_list:
        df = pd.read_csv(file)
        dfs.append(df)
    
    try:
        merged_df = pd.concat(dfs, ignore_index=True)
    except Exception: # if there is only one final_ csv file
        merged_df = df
    
    merged_df["sessions"] = merged_df["dir"].apply(lambda x: os.path.basename(os.path.dirname(os.path.dirname(x))))
    merged_df.sort_values(by='sessions', inplace=True)
    merged_df = merged_df.drop(columns='sessions')
    
    merged_csv_path = os.path.join(directory_path, 'all_sessions_annotation.csv')
    
    merged_df.to_csv(merged_csv_path, index=False, header=True)

def copy_and_zip_folder(src_folder, dest_folder, session_name):
    '''
    Function to temporarily copy the session folder and zip every folders in it plus subfolders of PROCESSED_DATA.
    You will get as output a folder with .zip archives in it.
    '''
    session_folder = os.path.join(dest_folder, session_name)

    # Copy the source folder to the destination folder
    shutil.copytree(src_folder, session_folder, dirs_exist_ok=True)

    # List subdirectories in the session folder
    subdirs = [os.path.join(session_folder, subdir) for subdir in os.listdir(session_folder) if os.path.isdir(os.path.join(session_folder, subdir))]

    # Zip subfolders:
    for subdir in subdirs:
        subdir_name = os.path.basename(subdir)

        if subdir_name in ["DCIM", "DCIM_THUMBNAILS", "GPS", "LABEL", "METADATA", "PROCESSED_DATA", "SENSORS"]:
            if subdir_name == "PROCESSED_DATA": # zipping folders in PROCESSED_DATA
                subsubdirs = [os.path.join(subdir, subsubdir) for subsubdir in os.listdir(subdir) if os.path.isdir(os.path.join(subdir, subsubdir))]
                for subsubdir in subsubdirs:
                    subsubdir_name = os.path.basename(subsubdir)
                    zip_filename = os.path.join(subdir, f"{subsubdir_name}.zip")
                    with zipfile.ZipFile(zip_filename, "w", zipfile.ZIP_DEFLATED) as zipf:
                        for root, dirs, files in os.walk(subsubdir):
                            for file in files:
                                file_path = os.path.join(root, file)
                                arcname = os.path.relpath(file_path, subsubdir)
                                zipf.write(file_path, arcname)

                    shutil.rmtree(subsubdir)

            # zipping every subdir
            zip_filename = os.path.join(session_folder, f"{subdir_name}.zip")
            with zipfile.ZipFile(zip_filename, "w", zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(subdir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, subdir)
                        zipf.write(file_path, arcname)

            # Delete the original subdirectory and its contents
            shutil.rmtree(subdir)

    # Removing unwanted folders and files in the sesssion folder
    for item in os.listdir(session_folder):
        item_path = os.path.join(session_folder, item)
        
        # Check if the item is a file and has a .zip extension
        if os.path.isfile(item_path) and item.lower().endswith('.zip'):
            # Keep the zip file
            continue
        # If the item is a subfolder, remove it recursively
        elif os.path.isdir(item_path):
            try:
                shutil.rmtree(item_path)
            except Exception as e:
                print(f"Error deleting folder {item_path}: {e}")
        # If the item is a non-zip file, remove it
        else:
            try:
                os.remove(item_path)
            except Exception as e:
                print(f"Error deleting file {item_path}: {e}")

def create_sessions_stats(session, session_name, jacques_model_path):
    '''
    Function that creates a .txt file with jacques classification statistics 
    (total images, number of useful/useless images, percentage of useless images in relation of total images)
    '''
    jacques_model = jacques_model_path.split("/")[-1]
    jacques_model = jacques_model.split(".")[0]
    input_file = os.path.join(session, f'LABEL/classification_{session_name}_jacques-v0.1.0_model-{jacques_model}.csv')
    output_file = os.path.join(session, f'METADATA/{session_name}_stats.txt')

    if not os.path.exists(os.path.dirname(input_file)):
        input_file = os.path.join(session, f'PROCESSED_DATA/IA/classification_{session_name}_jacques-v0.1.0_model-{jacques_model}.csv')
    
    df = pd.read_csv(input_file)
    total_images = len(df)
    useful_images = len(df[df['class'] == 'useful'])
    useless_images = len(df[df['class'] == 'useless'])
    percentage_useless = (useless_images / total_images) * 100
    
    with open(output_file, 'w') as f:
        f.write(f"Total images: {total_images}\n")
        f.write(f"Useful images: {useful_images}\n")
        f.write(f"Useless images: {useless_images}\n")
        f.write(f"Percentage of useless images: {percentage_useless:.2f}%\n")
    
    print(f"{session_name} statistics written to {output_file}.")

def resize_images(source_folder, destination_folder, file_name):
    '''
    Function to resize images to thumbnail size (224x224).
    '''
    destination_path = os.path.join(destination_folder, file_name)
    image_path = os.path.join(source_folder, file_name)
    image = Image.open(image_path)
    # Resize the images where shortest side is 256 pixels, keeping aspect ratio. 
    if image.width > image.height: 
        factor = image.width/image.height
        image = image.resize(size=(int(round(factor*256,0)),256))
    else:
        factor = image.height/image.width
        image = image.resize(size=(256, int(round(factor*256,0))))
    # Crop out the center 224x224 portion of the image.

    image = image.crop(box=((image.width/2)-112, (image.height/2)-112, (image.width/2)+112, (image.height/2)+112))

    image.save(destination_path)

def create_thumbnails_for_FRAMES(session):
    '''
    Create thumbnails for images located in the folder /PROCESSED_DATA/FRAMES/
    '''
    images_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
    file_list = os.listdir(images_folder_path)
    destination_folder = os.path.join(session, 'DCIM_THUMBNAILS/')
    os.makedirs(destination_folder, exist_ok=True)
    for file_name in file_list:
        try:
            resize_images(images_folder_path, destination_folder, file_name)
        except Exception as e:
            print(f"[ERROR] Failed to create thumbnail of {file_name}: {e}")


def create_thumbnails(session):
    '''
    This function creates a folder THUMBNAILS and save in it resized images of the processed session.
    '''
    images_folder_path = f'{session}/DCIM/'
    if os.path.exists(images_folder_path):
        list_of_dir = get_subfolders(f'{session}/DCIM/')
        if len(list_of_dir) != 0:
            for directory in list_of_dir:
                directory_name = directory.split("/")[-1]
                print(f"Creating thumbnails for directory {directory_name}")
                destination_folder = os.path.join(session, "DCIM_THUMBNAILS/")
                os.makedirs(destination_folder, exist_ok=True)
                folder_path = os.path.join(session, directory)
                file_list = os.listdir(folder_path)
                for file_name in file_list:
                    try:
                        resize_images(folder_path, destination_folder, file_name)
                    except Exception as e:
                        print(f"[ERROR] Failed to create thumbnail of {file_name}: {e}")
        else:
            create_thumbnails_for_FRAMES(session)        
    else:
        create_thumbnails_for_FRAMES(session)
        

def process_frames(session, results_of_all_sessions, directory, jacques_model_path):
    '''
    Function to launch jacques classification on images located in the folder /PROCESSED_DATA/FRAMES/
    '''
    print("\n[Frames processing]")
    folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
    file_list = os.listdir(folder_path)

    for file_name in file_list:
        if file_name.startswith('._'): # removing hidden images that starts with '._'
            file_path = os.path.join(folder_path, file_name)
            try:
                os.remove(file_path)
                print(f"\nRemoved unidentified image: {file_name}")
            except OSError as e:
                print(f"\nError removing image {file_name}: {e}")

    try:
        results = predictor.classify_useless_images(folder_path=folder_path, ckpt_path=jacques_model_path)
        results_of_all_sessions = pd.concat([results_of_all_sessions, results], axis=0, ignore_index=True)
        return results_of_all_sessions
    except Exception as e:
        print(f"\n[ERROR] Classification error in {directory}: {e}")
        pass

def evenly_select_images_on_interval(image_list):
    '''
    Function to select images evenly throughout a list based on their indexes.
    '''
    total_images = len(image_list)
    index_list = np.linspace(0, total_images, 100, dtype=int, endpoint=False)
    selected_images = [image_list[i] for i in index_list]
    return selected_images

def create_trajectory_map(metadata_path, global_trajectories):
    '''
    Function to create the trajectory maps.
    - metadata_path is the path to the metadata.csv file or the metadata_image.csv file.
    - global_trajectories is a boolean to indicate if you are doing the global trajectory map or not.
    '''
    df = pd.read_csv(metadata_path)

    imagery = GoogleTiles(url='https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}')

    if not global_trajectories:
        fig = plt.figure(figsize=(2,2), dpi=300)
        ax = fig.add_subplot(projection=ccrs.PlateCarree())
        map_path = "map.png"
        adb_lat_range = [-13, -7] # latitude range of aldabra and mayotte
        if df['GPSLatitude'].between(adb_lat_range[0], adb_lat_range[1]).any():
            ax.set_extent([df.GPSLongitude.min()-0.005, df.GPSLongitude.max()+0.005, df.GPSLatitude.min()-0.005,df.GPSLatitude.max()+0.005])
            ax.add_image(imagery, 17) # aldabra/mayotte position so we adjust the zoom level
            ax.plot(df.GPSLongitude, df.GPSLatitude, color='yellow', linewidth=0.1)
        else: # other positions
            ax.set_extent([df.GPSLongitude.min()-0.005, df.GPSLongitude.max()+0.005, df.GPSLatitude.min()-0.001,df.GPSLatitude.max()+0.001])
            ax.add_image(imagery, 19)
            ax.plot(df.GPSLongitude, df.GPSLatitude, color='yellow', linewidth=0.3)
    else: # global trajectories map
        df['GPSDateTime'] = pd.to_datetime(df['GPSDateTime'])
        df['SubSecDateTimeOriginal'] = pd.to_datetime(df['SubSecDateTimeOriginal'], format="%Y:%m:%d %H:%M:%S.%f")
        df['Date'] = df['GPSDateTime'].fillna(df['SubSecDateTimeOriginal'])
        df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')

        fig = plt.figure(figsize=(10,10), dpi=600)
        ax = fig.add_subplot(projection=ccrs.PlateCarree())
        ax.set_extent([df.GPSLongitude.min()-1, df.GPSLongitude.max()+1, df.GPSLatitude.min()-1,df.GPSLatitude.max()+1])
        ax.add_image(imagery, 10)

        # drawing trajectories for each session based on their associated dates
        for date in df["Date"].unique():
            subset = df[df['Date'] == date]
            ax.plot(subset.GPSLongitude, subset.GPSLatitude, color='yellow', linewidth=5)
        # saving the map in the same folder as metadata_image.csv
        map_path = os.path.join(os.path.dirname(metadata_path), "000_global_map.png")

    fig.savefig(map_path, bbox_inches='tight',pad_inches=0, dpi=600)
    print("Trajectory map created!")

def create_pdf_preview(pdf_preview_path, session, session_name, list_of_images):
    '''
    Function to create a pdf preview of the session. It will contains:
    - a trajectory map
    - 100 thumbnails of images selected evenly throughout the session
    - a sneakpeek to the metadata file of the session
    '''

    # PDF creation
    pdf_file = os.path.join(pdf_preview_path, f"000_{session_name}_preview.pdf")
    c = canvas.Canvas(pdf_file, pagesize=letter)
    page_width, page_height = letter
    max_height = page_height - 100

    c.setFont("Helvetica-Bold", 14)
    c.drawString(30, 730, "Session Summary")
    c.setFont("Helvetica-Bold", 18)
    c.setFillColor(colors.blue)
    c.drawString(30, 705, session_name)

    # Trajectory map
    metadata_path = os.path.join(session, "METADATA/metadata.csv")

    if not os.path.exists(metadata_path):
        print("\nMetadata file not found for trajectory map creation!")

    create_trajectory_map(metadata_path, False)
    print("Adding map to the PDF...")
    c.drawImage("map.png", 20, 300)
    os.remove("map.png") # deleting map.png

    c.setFont("Helvetica-Bold", 16)
    c.setFillColor(colors.black)
    c.drawString(30, 650, "Trajectory map")
    print("Map added!")

    # Thumbnails
    c.showPage()
    c.setFont("Helvetica-Bold", 16)
    c.drawString(30, 730, "Images previews")

    selected_images = evenly_select_images_on_interval(list_of_images)
    print("Images previews selected!\n")
    x_coord = 30
    y_coord = max_height

    for i, image in enumerate(selected_images):
        if i % 5 == 0 and i != 0:
            # Start a new row of images
            x_coord = 30
            y_coord -= 110

        img = Image.open(image)
        img.thumbnail((100, 100))

        img_width, img_height = img.size

        temp_image_path = os.path.join(pdf_preview_path, f'temp_{i}.jpg')
        img.save(temp_image_path)

        if y_coord - img_height < 50:
            c.showPage()
            y_coord = max_height

        c.drawImage(temp_image_path, x_coord, y_coord - img_height)

        os.remove(temp_image_path)

        x_coord += 110

    c.showPage()
    c.setPageSize(landscape(letter))

    # Metadata preview
    print("Loading data for metadata preview...")
    metadata_file = os.path.join(session, f"METADATA/metadata.csv")
    df = pd.read_csv(metadata_file)
    print("Data loaded!")
    preview_df = df.head(20)
    print("Preview dataframe created!")

    try:
        preview_df = preview_df[["FileName", "SubSecDateTimeOriginal", "GPSLatitude", "GPSLongitude", "FileSize", "ImageHeight", "ImageWidth"]]
    except KeyError:
        preview_df = preview_df[["photo_identifier", "GPSDateTime", "GPSLatitude", "GPSLongitude", "FileSize", "ImageHeight", "ImageWidth"]]

    print("Creation of the PDF table...")
    table_data = [list(preview_df.columns)] + preview_df.values.tolist()
    table = Table(table_data)

    table.setStyle([
        ('TEXTCOLOR', (0, 0), (-1, 0), (0, 0, 1)),  # Header row text color (blue)
        ('FONTSIZE', (0, 1), (-1, -1), 8), # Font size of all cells
        ('BACKGROUND', (0, 0), (-1, 0), (0.7, 0.7, 0.7)),  # Header row background color (gray)
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),  # Center align all cells
        ('INNERGRID', (0, 0), (-1, -1), 0.25, (0, 0, 0)),  # Inner gridlines
        ('BOX', (0, 0), (-1, -1), 0.25, (0, 0, 0)),  # Cell borders
    ])

    table.wrapOn(c, 10, 20)
    table.drawOn(c, 30, 100)
    print("PDF table sucessfully created!")
    c.setFont("Helvetica-Bold", 16)
    c.drawString(30, 530, "Metadata preview")
    
    c.save()
    print("PDF created!")

def add_useful_column(df, classification_path):
    '''
    Function to add the "Useful" column in the metadata_image.csv file.
    '''
    files = os.listdir(classification_path)
    for file in files:
        # getting the jacques classification csv file based on the filename
        if file.startswith("classification"):
            file_path = os.path.join(classification_path, file)
            classification_df = pd.read_csv(file_path)
            # merge based on the 'FileName' column of metadata.csv and 'image' column of jacques classification
            merged_df = df.merge(classification_df, left_on="FileName", right_on="image", how="left")
            # creation of 'Useful' column from the 'class' column. Replacing 'useful' with 1 and 'useless' with 0.
            merged_df["Useful"] = merged_df["class"].apply(lambda x: 1 if x == "useful" else 0)
            # dropping unused columns
            merged_df.drop(["image", "class", "dir"], axis=1, inplace=True)
    return merged_df

def create_global_stats(metadata_image_path):
    '''
    Function to create a .txt file that contains statistics about the metadata_image.csv file.
    '''
    df = pd.read_csv(metadata_image_path)
    total_images = len(df)

    if "Useful" in df.columns:
        output_file = os.path.join(os.path.dirname(metadata_image_path), "metadata_image_stats.txt")

        useful_images = len(df[df['Useful'] == 1])
        useless_images = len(df[df['Useful'] == 0])
        percentage_useless = (useless_images / total_images) * 100
        
        with open(output_file, 'w') as f:
            f.write(f"Total images: {total_images}\n")
            f.write(f"Useful images: {useful_images}\n")
            f.write(f"Useless images: {useless_images}\n")
            f.write(f"Percentage of useless images: {percentage_useless:.2f}%\n")
    else:
        with open(output_file, 'w') as f:
            f.write(f"Total images: {total_images}\n")

def create_metadata_image_csv(sessions, global_data_path):
    '''
    Function to create the metadata_image.csv file from the metadata.csv file of each session.
    '''

    list_of_sessions = get_sessions_list(sessions, -1)
    dfs = []
    newFileNameList = []

    for session in list_of_sessions:
        session_name = session.split('/')[-1]
        session_metadata_path = os.path.join(session, "METADATA/metadata.csv")
        if os.path.exists(session_metadata_path):
            df = pd.read_csv(session_metadata_path)

            # Adding "Useful" column if there are jacques classification informations
            classification_path = os.path.join(session, "LABEL/")
            if os.path.exists(classification_path):
                df = add_useful_column(df, classification_path)
            else:
                classification_path = os.path.join(session, "PROCESSED_DATA/IA/")
                if os.path.exists(classification_path):
                    df = add_useful_column(df, classification_path)

            if not df["FileName"][0].startswith(session_name): # checking if original filename is not already in the correct format
                newFileNameList.append([session_name +"_"+ filename for filename in df["FileName"]]) # correction of the filename based on session folder name
            else:
                newFileNameList.append(df["FileName"].tolist()) # keeping original filenames

            dfs.append(df)
        else:
            print("\n[ERROR] Metadata file not found!")


    if len(dfs) != 0:
        try:
            merged_df = pd.concat(dfs, ignore_index=True)
        except Exception: # if there is only one session
            merged_df = df

        merged_df = merged_df.rename(columns={"FileName":"OriginalFileName"})

        fileNameList = [item for sublist in newFileNameList for item in sublist]
        merged_df["FileName"] = fileNameList

        cols_to_keep = ["OriginalFileName", "FileName", "GPSLatitude", "GPSLongitude", "Useful", "ApertureValue", "Compression", "Contrast", "CreateDate", "DateCreated", "DateTimeDigitized", "DateTimeOriginal", 
                        "DigitalZoomRatio", "ExifImageHeight", "ExifImageWidth", "ExifToolVersion", "ExifVersion", "ExposureCompensation", "ExposureMode", "ExposureProgram", "FileSize", "FileType",  
                        "FileTypeExtension", "FNumber", "FocalLength", "FocalLength35efl", "FocalLengthIn35mmFormat", "FOV", "GPSAltitude", "GPSAltitudeRef", "GPSDateTime", "GPSDate", "GPSTime", "GPSLatitudeRef", 
                        "GPSLongitudeRef", "GPSMapDatum", "GPSPosition", "GPSTimeStamp", "GPSRoll", "GPSPitch", "GPSTrack", "ImageHeight", "ImageWidth", "LightValue", "Make", "MaxApertureValue", 
                        "MaximumShutterAngle", "Megapixels", "MeteringMode", "MIMEType", "Model", "Saturation", "ScaleFactor35efl", "SceneCaptureType", "SceneType", "SensingMethod", "Sharpness", 
                        "ShutterSpeed", "Software", "SubSecDateTimeOriginal", "ThumbnailImage", "ThumbnailLength", "ThumbnailOffset", "WhiteBalance", "XResolution", "YResolution","GPSfix", "GPSsdne", "GPSsde", "GPSsdn"]
        
        filtered_columns = [col for col in cols_to_keep if col in merged_df.columns] # only keep columns that exists in merged_df

        merged_df = merged_df[filtered_columns]

        merged_csv_path = os.path.join(global_data_path, 'metadata_image.csv')
        merged_df.to_csv(merged_csv_path, index=False, header=True)
        print("metadata_image.csv created!")
    

def classify_sessions(sessions, session_index, jacques_model_path):
    '''
    # Function that uses jacques predicator classify_useless_images function to classify given sessions images.
    ### Input:
        - sessions: 
            can be a string that refer to a single directory that contains 
            every sessions (ex: 'sessions/') or can be a list of the desired sessions (ex: ['sessions/session_2017_11_05_kite_Le_Morne'])
        - session_index:
            index of the session to process
        - jacques_model_path:
            path to jacques model
    ### Output:
        - a dataframe that contains the classification result for the selected sessions images with columns 'dir', 'image' and 'class'
        
    '''
    list_of_sessions = get_sessions_list(sessions, session_index)
    
    # classification of useful and useless images of all sessions
    results_of_all_sessions = pd.DataFrame(columns = ['dir', 'image', 'class'])
    for session in list_of_sessions:
        print("-----------------------------------------------")
        print(session)
        print("-----------------------------------------------")
        dcim_path = f'{session}/DCIM/'
        if os.path.exists(dcim_path):
            list_of_dir = get_subfolders(dcim_path)
            if len(list_of_dir) != 0:
                print("\n[Images processing]")
                for directory in list_of_dir:
                    print('\n' + directory)
                    folder_path = os.path.join(session, directory)
                    file_list = os.listdir(folder_path)
                    for file_name in file_list:
                        if file_name.startswith('._'):
                            file_path = os.path.join(folder_path, file_name)
                            try:
                                os.remove(file_path)
                                print(f"\nRemoved unidentified image: {file_name}")
                            except OSError as e:
                                print(f"\nError removing image {file_name}: {e}")

                    try:
                        results = predictor.classify_useless_images(folder_path=folder_path, ckpt_path=jacques_model_path)
                        results_of_all_sessions = pd.concat([results_of_all_sessions, results], axis=0, ignore_index=True)
                    except Exception as e:
                        print(f"\n[ERROR] Classification error in {directory}: {e}")
                        pass
            else: # if there are no folders in DCIM
                dcim_path = f'{session}/PROCESSED_DATA/FRAMES/'
                results_of_all_sessions = process_frames(session, results_of_all_sessions, dcim_path, jacques_model_path)
        else: # if DCIM doesn't exists, it means we are dealing with frames
            dcim_path = f'{session}/PROCESSED_DATA/FRAMES/'
            results_of_all_sessions = process_frames(session, results_of_all_sessions, dcim_path, jacques_model_path)
        
                    

    return results_of_all_sessions

def restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, pdf_preview, global_data_path, delete_before_after_useless, global_trajectory_map):
    '''
    # Function to restructure sessions folders to be "Zenodo ready"
    ### Input:
        - sessions:
            it's the list of sessions composed of either a single directory that contains every sessions. (ex: '/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_unzipped/')
            or a list of sessions path (ex: ['/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_unzipped/session_2017_11_19_paddle_Black_Rocks'])
        - session_index:
            it's the index of the session that is being processed 
        - zipped_sessions_path:
            it's the path to the folder where you want to create the zipped version of each session (ex:'/my/path/to/zip_folder/') 
        - dest_path:
            destination path where useless images will be moved. (ex: '/home3/datawork/aboyer/mauritiusSessionsOutput/useless_images/')
        - annot_path:
            path where multilabel annotations csv will be created. (ex: '/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_processing_output/results_csv/herbier_classification/herbier_classification_.csv')
        - jacques_model_path:
            path to jacques model. (ex: '/home/datawork-iot-nos/Seatizen/models/useless_classification/version_17/checkpoints/epoch=7-step=2056.ckpt')
        - annotation_model_path:
            path to annotation model. (ex: '/home/datawork-iot-nos/Seatizen/mauritius_use_case/Mauritius/models/multilabel_with_sable.pth')
        - thresholds_labels:
            a dictionnary that contains labels names and their associated thresholds
        - pdf_preview:
            boolean value to indicate if a pdf preview will be created for each session
        - global_data_path:
            path where metadata_image.csv will be created
        - delete_before_after_useless:
            boolean value to indicate if the user want to delete before, after and useless folders if they exists
        - global_trajectory_map:
            boolean value to indicate if the user want to create the global trajectory map or not
    ### Output:
        - sessions folders "Zenodo ready"
    '''
    if len(sessions) != 0:
        # jacques classification
        if len(jacques_model_path) != 0:
            results_of_all_sessions = classify_sessions(sessions, session_index, jacques_model_path)
        else:
            results_of_all_sessions = pd.DataFrame(data={}) # empty dataframe
        
        list_of_sessions = get_sessions_list(sessions, session_index)

        # global data
        if len(global_data_path) != 0:
            metadata_image_path = os.path.join(global_data_path, "metadata_image.csv")
            if not global_trajectory_map:
                print("\nCreation of metadata_image.csv from all metadata files...")
                create_metadata_image_csv(sessions, global_data_path)
                if os.path.exists(metadata_image_path):
                    create_global_stats(metadata_image_path) # creation of a txt file with statistics about the metadata_image.csv
            else:
                if os.path.exists(metadata_image_path):
                    create_trajectory_map(metadata_image_path, True)
        
        # operations on each session
        for session in list_of_sessions:
            session_name = session.split('/')[-1]

            if not results_of_all_sessions.empty:
                # export results to csv file
                model = jacques_model_path.split('/')[-1]
                # adding session name, jacques version and classification model version to csv filename
                suffix = f"{session_name}_jacques-v0.1.0_model-{model.split('.')[0]}"
                class_path = os.path.join(session, 'LABEL/classification_.csv')

                if not os.path.exists(os.path.dirname(class_path)):
                    class_path = os.path.join(session, 'PROCESSED_DATA/IA/')
                    os.makedirs(class_path, exist_ok=True)
                    class_path = os.path.join(class_path, 'classification_.csv')

                class_path = class_path[:-4] + suffix + class_path[-4:]
                results_of_all_sessions.to_csv(class_path, index = False, header = True)
                print(f'\nClassification informations written at {class_path}\n')

                # create a .txt file in the METADATA folder of each session. This file give statistics about the jacques classification result.
                try:
                    print(f"Creation of {session_name} jacques stats file...")
                    create_sessions_stats(session, session_name, jacques_model_path)
                except Exception as e:
                    print(e)
            
            # move useless images
            if len(dest_path) != 0:
                dest_path = os.path.join(dest_path, session_name)
                move_out_images(class_path, dest_path, who_moves='useless')
                print(f"All useless images successfully moved to {dest_path}!")
            
            image_folder_path = f'{session}/DCIM/'
            if os.path.exists(image_folder_path):
                # checking if there are "GOPRO" folders in DCIM
                list_of_dir = get_subfolders(image_folder_path)
                if len(list_of_dir) == 0:
                    image_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
                else:
                    # delete BEFORE, AFTER and USELESS folders if they exists
                    if delete_before_after_useless:
                        delete_folders(image_folder_path) 
            else:
                image_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
            
            for key, path in annot_path.items():
                if len(path) != 0:
                    print(f"\nStarting annotations using {key} model.")
                    csv_annotation_path = path[:-4] + session_name + path[-4:]
                    final_csv_path = os.path.join(os.path.dirname(csv_annotation_path), "final_annotation_.csv")
                    final_csv_path = final_csv_path[:-4] + session_name + final_csv_path[-4:]

                    try:
                        model_path = annotation_model_path.get(key)
                    except KeyError:
                        print(f"{key} not found in annotation_model_path definition in config.json file.")
                    try:
                        model_thresholds_labels = threshold_labels.get(key)
                    except KeyError:
                        print(f"{key} not found in threshold_labels definition in config.json file.")

                    # adding model annotations
                    
                    list_of_dir = get_subfolders(image_folder_path)
                    list_of_df = []

                    if len(list_of_dir) != 0:
                        for directory in list_of_dir:
                            print(f"\nAnnotations of images located in {directory}...")
                            images_path = os.path.join(session, directory)
                            df = annotation_model(images_path, csv_annotation_path, model_path, model_thresholds_labels)
                            list_of_df.append(df)

                        df = pd.concat(list_of_df, ignore_index=True)
                    else:
                        df = annotation_model(image_folder_path, csv_annotation_path, model_path, model_thresholds_labels)
                        
                    df.sort_values(by="image", inplace=True)
                    df.to_csv(csv_annotation_path, index = False, header = True)
                    print(f'\nAnnotations CSV of {session} successfully created at {csv_annotation_path}\n')
                    
                    # join GPS metadata to annotation file
                    metadata_path = f'{session}/METADATA/metadata.csv'
                    if os.path.exists(metadata_path):
                        join_GPS_metadata(csv_annotation_path, metadata_path, final_csv_path)
                        print(f'Merged GPS metadata with annotation results at {final_csv_path}\n')
                    
                    # merge all sessions final annotation
                    merge_all_final_csv(csv_annotation_path)

            if len(zipped_sessions_path) != 0:
                if not pdf_preview: # zip sessions folders
                    print(f"Zipping {session_name}...")
                    try:
                        create_thumbnails(session)
                        copy_and_zip_folder(session, zipped_sessions_path, session_name)
                        print(f"\n{session_name} successfully zipped.")
                        shutil.rmtree(os.path.join(session, "DCIM_THUMBNAILS"))
                    except Exception as e:
                        print(e)
                else: # create pdf previews in the folders where the zip of each session are
                    print(f"\nCreation of pdf preview for session {session_name}...")
                    pdf_preview_path = os.path.join(zipped_sessions_path, session_name)
                    os.makedirs(pdf_preview_path, exist_ok=True)
                    list_of_dir = get_subfolders(image_folder_path)
                    list_of_images = []
                    if len(list_of_dir) != 0:
                        for directory in list_of_dir:
                            images_path = os.path.join(session, directory)
                            images_files = [f for f in os.listdir(images_path) if f.endswith('.JPG') or f.endswith('.jpg') or f.endswith('.jpeg')]
                            images_files.sort()
                            selected_images = [os.path.join(f'{images_path}/', image_name) for image_name in images_files]
                            list_of_images.append(selected_images)
                        list_of_images = [item for sublist in list_of_images for item in sublist]
                        create_pdf_preview(pdf_preview_path, session, session_name, list_of_images)
                    else:
                        list_of_images = [f for f in os.listdir(image_folder_path) if f.endswith('.JPG') or f.endswith('.jpg') or f.endswith('.jpeg')]
                        list_of_images.sort()
                        list_of_images = [os.path.join(image_folder_path, image_name) for image_name in list_of_images]
                        create_pdf_preview(pdf_preview_path, session, session_name, list_of_images)

    else:
        print("[ERROR] You must fill in a path to your sessions in the config.json file! Refer to the README.md file for more informations.")
            
        
def main():
    start_time = time.time()
    print(f"Start time: {datetime.now()}")
    # getting session index from script args
    parser = argparse.ArgumentParser()
    parser.add_argument("--session-index",
                        action="store",
                        type=int,
                        default=argparse.SUPPRESS,
                        help="Index of the session that is being processed.")
    args = parser.parse_args()
    
    # read the config.json file
    with open('config.json') as json_file:
        config = json.load(json_file)
    
    # getting all variables from config.json file
    ## models
    jacques_model_path = config["paths"]["jacques_model_path"]
    annotation_model_path = config["paths"]["annotation_model_path"]
    ## paths
    sessions = config["paths"]["sessions_path"]
    zipped_sessions_path = config["paths"]["zipped_sessions_path"]
    global_data_path = config["paths"]["global_data_path"]
    dest_path = config["paths"]["useless_images_path"]
    annot_path = config["paths"]["annotation_csv_path"]
    ## threshold labels dictionnary
    threshold_labels = config["threshold_labels"]
    ## booleans
    delete_before_after_useless = config["delete_before_after_useless"]
    global_trajectory_map = config["global_trajectory_map"]
    pdf_preview = config["pdf_preview"]

    if hasattr(args, "session_index"):
        session_index = args.session_index - 1
        restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, pdf_preview, global_data_path, delete_before_after_useless, global_trajectory_map)
        execution_time_seconds = time.time() - start_time
        execution_time_minutes = "{:.2f}".format(execution_time_seconds / 60)
        print("\n========================================================")
        print(f"\nEnd time: {datetime.now()}")
        print(f"Total execution time: {execution_time_minutes} minutes")
    else: # execution on all sessions in the folder
        session_index = -1
        restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, pdf_preview, global_data_path, delete_before_after_useless, global_trajectory_map)
        execution_time_seconds = time.time() - start_time
        execution_time_minutes = "{:.2f}".format(execution_time_seconds / 60)
        print("\n========================================================")
        print(f"\nEnd time: {datetime.now()}")
        print(f"Total execution time: {execution_time_minutes} minutes")

if __name__ == '__main__':
    main()
