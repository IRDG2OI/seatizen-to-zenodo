#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jul 10 15:04:46 2023

@author: g2oi
"""

from jacques.inference import predictor
import os
import pandas as pd
import shutil
import argparse
import time
from datetime import datetime
import glob
from predict import annotation_model
import json
import zipfile
from PIL import Image

# get folder list that contains images (['DCIM/100GOPRO/', 'DCIM/101GOPRO/'])
def get_subfolders(folder_path):
    subfolders = []
    for item in os.listdir(folder_path):
        item_path = os.path.join(folder_path, item)
        if os.path.isdir(item_path) and 'GOPRO' in item:
            subfolder_name = f"DCIM/{os.path.basename(item_path)}"
            subfolders.append(subfolder_name)
    return subfolders

# List ONLY directories
def list_directories(path):
    directories = [entry for entry in os.listdir(path) if os.path.isdir(os.path.join(path, entry))]
    return directories

# Delete "BEFORE" and "AFTER" folders at specified path if they exists
def delete_folders(path):
    before_path = os.path.join(path, 'BEFORE')
    after_path = os.path.join(path, 'AFTER')

    # Check if "BEFORE" folder exists and delete it
    if os.path.exists(before_path) and os.path.isdir(before_path):
        shutil.rmtree(before_path)
        print(f'Deleted folder: {before_path}')

    # Check if "AFTER" folder exists and delete it
    if os.path.exists(after_path) and os.path.isdir(after_path):
        shutil.rmtree(after_path)
        print(f'Deleted folder: {after_path}')

# return the list of sessions
def get_sessions_list(sessions, session_index):
    if(isinstance(sessions, str)): # if it's a string, it's one directory
        # list sessions automatically from one directory
        directory_of_sessions = sessions
        list_of_sessions = list_directories(directory_of_sessions)
        list_of_sessions = [os.path.join(directory_of_sessions, session) for session in list_of_sessions]
        list_of_sessions.sort()
        if session_index != -1:
            list_of_sessions = [list_of_sessions[session_index]]
    elif(isinstance(sessions, list)): # if it's a list, sessions are written by hand
        if session_index != -1:
            list_of_sessions = [sessions[session_index]]
        else:
            list_of_sessions = sessions
    
    return list_of_sessions

def move_out_images(csv_file, destination_directory, who_moves=['useless', 'useful']):
    '''
    Function that moves images predicted as useless or useful in the destination path.
    # Input:
    - csv_file : the path to the classification csv generated by jacques
    - destination_directory : the path where the images will be moved
    - who_moves : whether to move the useful or useless images in another directory
    # Output:
    Images are moved into the destination path
    '''
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file)
    
    # Filter rows with the chosen class
    useless_images = df[df["class"] == who_moves]
    
    # Create destination directory if it doesn't exists
    os.makedirs(destination_directory, exist_ok=True)
    
    # Move images to another directory
    for _, row in useless_images.iterrows():
        image_filename = row["image"]
        source_directory = row["dir"]  # Extract source directory from the 'dir' column
        source_path = os.path.join(source_directory, image_filename)
        destination_path = os.path.join(destination_directory, image_filename)

        # Check if the image exists in the source directory before moving
        if os.path.exists(source_path):
            shutil.move(source_path, destination_path)
            print(f"Moved image '{image_filename}' to '{destination_directory}'.")
        else:
            print(f"Image '{image_filename}' not found in '{source_directory}'.")

# move back useless images to their original folders
def move_back_images(csv_path, useless_img_path):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_path)
    
    # Iterate over each row in the DataFrame
    for index, row in df.iterrows():
        image_path = useless_img_path + row['image']
        image_class = row['class']
        original_dir = row['dir']
    
        # Check if the image class is 'useless'
        if image_class == 'useless':
            # Create the original directory if it doesn't exist
            # os.makedirs(original_dir, exist_ok=True)
            # Move the image back to its original directory
            shutil.move(image_path, original_dir)
    
            # Display the operation for confirmation
            print(f"Moved image '{image_path}' back to '{original_dir}'.")

# merge multilabel annotations csv with GPS metadata (latitude, longitude and date)
def join_GPS_metadata(annotation_csv_path, gps_info_csv_path, merged_csv_path):
    annot_df = pd.read_csv(annotation_csv_path)
    gps_df = pd.read_csv(gps_info_csv_path)
    
    # Extract image names from the file paths
#     annot_df['Image_name'] = annot_df['Image_path'].str.split('/').str[-1]
    annot_df['Image_name'] = annot_df['image']
    try:
        gps_df['Image_name'] = gps_df['photo_relative_file_path'].str.split('/').str[-1]
    except KeyError:
        gps_df['Image_name'] = gps_df['FileName']
    
    # Merge the DataFrames based on the image names
    try:
        merged_df = annot_df.merge(gps_df[['Image_name', 'decimalLatitude', 'decimalLongitude', 'GPSDateTime']],
                                on='Image_name', how='left')
    except KeyError:
        merged_df = annot_df.merge(gps_df[['Image_name', 'Composite:GPSLatitude', 'Composite:GPSLongitude', 'SubSecDateTimeOriginal']],
                                on='Image_name', how='left')
    
    # Drop the 'Image_name' column from merged_df
    merged_df.drop(columns='Image_name', inplace=True)
    
    # Swapping latitude and longitude
    try:
        merged_df = merged_df.rename(columns = {'decimalLatitude':'decimalLongitude', 'decimalLongitude':'decimalLatitude'})
    except KeyError:
        pass
    
    merged_df.to_csv(merged_csv_path, index=False, header=True)
    
# get a list of merged files from a given path
def get_merged_files(path):
    merged_files = []
    
    # Get a list of all files and directories in the given path
    files_and_dirs = os.listdir(path)
    
    # Filter out the files that start with 'merged'
    for file in files_and_dirs:
        if file.startswith('merged') and os.path.isfile(os.path.join(path, file)):
            merged_files.append(file)
    
    return merged_files

# apply probabilities thresholds to the values of multilabel annotations
def apply_thresholds(merged_csv_path, thresholds_csv_path, final_csv_path):
    df_1 = pd.read_csv(merged_csv_path)
    df_2 = pd.read_csv(thresholds_csv_path)
    
    for column in df_1.columns:
        if column in df_2.columns:
            threshold_value = df_2[column][0]
            df_1[column] = df_1[column].apply(lambda x: x if x > threshold_value else 0)
            
    df_1.to_csv(final_csv_path, index=False, header=True)
    
# filter out useless images in the final csv based on jacques classification csv
def filter_useless_images(classification_csv, final_csv_path):
    df_1 = pd.read_csv(classification_csv)
    df_2 = pd.read_csv(final_csv_path)
    
    # Extract image name from image path
    df_2['image'] = df_2['Image_path'].str.split('/').str[-1]
    
    # Filter out the 'useless' images from the df_1
    df1_useful = df_1[df_1['class'] == 'useful']
    
    # Merge the two DF on the 'image' column and keep only rows that exist in both DF
    final_df = pd.merge(df_2, df1_useful, on='image', how='inner')
    
    # Removing useless columns
    final_df = final_df.drop(columns=['dir', 'class', 'image'])
    
    final_csv_path = final_csv_path[:-4] + '_filtered' + final_csv_path[-4:]
    
    final_df.to_csv(final_csv_path, index=False, header=True)

# merge all final csv files starting with 'final' located at csv_path in one csv file
def merge_all_final_csv(csv_path):
    directory_path = os.path.dirname(csv_path)
    wildcard_pattern = os.path.join(directory_path, 'final_*.csv')
    file_list = glob.glob(wildcard_pattern)
    
    dfs = []
    
    for file in file_list:
        df = pd.read_csv(file)
        dfs.append(df)
    
    merged_df = pd.concat(dfs, ignore_index=True)
    
    merged_df["sessions"] = merged_df["dir"].apply(lambda x: os.path.basename(os.path.dirname(os.path.dirname(x))))
    merged_df.sort_values(by='sessions', inplace=True)
    merged_df = merged_df.drop(columns='sessions')
    
    merged_csv_path = os.path.join(directory_path, 'all_sessions_annotation.csv')
    
    merged_df.to_csv(merged_csv_path, index=False, header=True)

def unzip(zip_file_index, source_folder, destination_folder):
    zip_files = [file for file in os.listdir(source_folder) if file.endswith('.zip')]
    zip_files.sort()
    zip_file = zip_files[zip_file_index]
    zip_file_path = os.path.join(source_folder, zip_file)
    session_name = zip_file[:-4]
    destination_folder = os.path.join(destination_folder, session_name)
    os.makedirs(destination_folder, exist_ok=True)
    
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(destination_folder)
        
def zip_folders(session, session_name, destination_folder):
    zip_filename = session_name + ".zip"
    zip_path = os.path.join(destination_folder, zip_filename)
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Walk through the folder and add each file and subfolder to the ZIP file
        for root, dirs, files in os.walk(session):
            for file in files:
                file_path = os.path.join(root, file)
                # The second argument of 'zipf.write' is the relative path inside the ZIP
                zipf.write(file_path, os.path.relpath(file_path, session))

    print(f"{session_name} zipped in {destination_folder} successfully.\n")

def copy_and_zip_folder(src_folder, dest_folder, session_name):
    session_folder = os.path.join(dest_folder, session_name)

    # Copy the source folder to the destination folder
    shutil.copytree(src_folder, session_folder)

    # List subdirectories in the session folder
    subdirs = [os.path.join(session_folder, subdir) for subdir in os.listdir(session_folder) if os.path.isdir(os.path.join(session_folder, subdir))]

    # Zip subfolders with specific names: DCIM, GPS, or PROCESSED_DATA
    for subdir in subdirs:
        subdir_name = os.path.basename(subdir)
        if subdir_name in ["DCIM", "GPS", "PROCESSED_DATA"]:
            zip_filename = os.path.join(session_folder, f"{subdir_name}.zip")
            with zipfile.ZipFile(zip_filename, "w", zipfile.ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(subdir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, subdir)
                        zipf.write(file_path, arcname)

            # Delete the original subdirectory and its contents
            shutil.rmtree(subdir)

    # Zip the entire copied folder
    zip_filename = os.path.join(dest_folder, f"{session_name}.zip")
    with zipfile.ZipFile(zip_filename, "w", zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(session_folder):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, session_folder)
                zipf.write(file_path, arcname)

    # Delete the copied folder
    shutil.rmtree(session_folder)

def create_sessions_stats(session, session_name, jacques_model_path):
    jacques_model = jacques_model_path.split("/")[-1]
    jacques_model = jacques_model.split(".")[0]
    input_file = os.path.join(session, f'LABEL/classification_{session_name}_jacques-v0.1.0_model-{jacques_model}.csv')
    output_file = os.path.join(session, f'METADATA/{session_name}_stats.txt')

    if not os.path.exists(os.path.dirname(input_file)):
        input_file = os.path.join(session, f'PROCESSED_DATA/IA/classification_{session_name}_jacques-v0.1.0_model-{jacques_model}.csv')
    
    df = pd.read_csv(input_file)
    total_images = len(df)
    useful_images = len(df[df['class'] == 'useful'])
    useless_images = len(df[df['class'] == 'useless'])
    percentage_useless = (useless_images / total_images) * 100
    
    with open(output_file, 'w') as f:
        f.write(f"Total images: {total_images}\n")
        f.write(f"Useful images: {useful_images}\n")
        f.write(f"Useless images: {useless_images}\n")
        f.write(f"Percentage of useless images: {percentage_useless:.2f}%\n")
    
    print(f"{session_name} statistics written to {output_file}.")

def resize_images(source_folder, destination_folder, file_name):
    destination_path = os.path.join(destination_folder, file_name)
    image_path = os.path.join(source_folder, file_name)
    image = Image.open(image_path)
    # Resize the images where shortest side is 256 pixels, keeping aspect ratio. 
    if image.width > image.height: 
        factor = image.width/image.height
        image = image.resize(size=(int(round(factor*256,0)),256))
    else:
        factor = image.height/image.width
        image = image.resize(size=(256, int(round(factor*256,0))))
    # Crop out the center 224x224 portion of the image.

    image = image.crop(box=((image.width/2)-112, (image.height/2)-112, (image.width/2)+112, (image.height/2)+112))

    image.save(destination_path)


def create_thumbnails(session):
    '''
    This function creates a folder THUMBNAILS and save in it resized images of the processed session.
    '''
    images_folder_path = f'{session}/DCIM/'
    if os.path.exists(images_folder_path):
        list_of_dir = get_subfolders(f'{session}/DCIM/')
        for directory in list_of_dir:
            directory_name = directory.split("/")[-1]
            print(f"Creating thumbnails for directory {directory_name}")
            destination_folder = os.path.join(session, f"THUMBNAILS/{directory_name}/")
            os.makedirs(destination_folder, exist_ok=True)
            folder_path = os.path.join(session, directory)
            file_list = os.listdir(folder_path)
            for file_name in file_list:
                try:
                    resize_images(folder_path, destination_folder, file_name)
                except Exception as e:
                    print(f"[ERROR] Failed to create thumbnail of {file_name}: {e}")
                
    else:
        images_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
        file_list = os.listdir(images_folder_path)
        destination_folder = os.path.join(session, 'PROCESSED_DATA/THUMBNAILS/')
        os.makedirs(destination_folder, exist_ok=True)
        for file_name in file_list:
            try:
                resize_images(images_folder_path, destination_folder, file_name)
            except Exception as e:
                print(f"[ERROR] Failed to create thumbnail of {file_name}: {e}")



    

def classify_sessions(sessions, session_index, jacques_model_path):
    '''
    Function that uses jacques predicator classify_useless_images function to classify given sessions images.
    # Input:
        - sessions: 
            ## can be a string that refer to a single directory that contains 
            every sessions (ex: 'sessions/')
            ## can be a list of the desired sessions (ex: ['sessions/session_2017_11_05_kite_Le_Morne'])
        - session_index:
            ## index of the session to process
        - jacques_model_path:
            ## path to jacques model
    # Output:
        - a dataframe that contains the classification result for the selected sessions images
        
    '''
    list_of_sessions = get_sessions_list(sessions, session_index)
    
    # classification of useful and useless images of all sessions
    results_of_all_sessions = pd.DataFrame(columns = ['dir', 'image', 'class'])
    for session in list_of_sessions:
        print("-----------------------------------------------")
        print(session)
        print("-----------------------------------------------")
        dcim_path = f'{session}/DCIM/'
        if os.path.exists(dcim_path):
            list_of_dir = get_subfolders(dcim_path)
            print("\n[Images processing]")
            for directory in list_of_dir:
                print('\n' + directory)
                folder_path = os.path.join(session, directory)
                file_list = os.listdir(folder_path)
                for file_name in file_list:
                    if file_name.startswith('._'):
                        file_path = os.path.join(folder_path, file_name)
                        try:
                            os.remove(file_path)
                            print(f"\nRemoved unidentified image: {file_name}")
                        except OSError as e:
                            print(f"\nError removing image {file_name}: {e}")

                try:
                    results = predictor.classify_useless_images(folder_path=folder_path, ckpt_path=jacques_model_path)
                    results_of_all_sessions = pd.concat([results_of_all_sessions, results], axis=0, ignore_index=True)
                except Exception as e:
                    print(f"\n[ERROR] Classification error in {directory}: {e}")
                    pass
        else: # if there are no folders in DCIM or if DCIM doesn't exists, it means we are dealing with frames
            print("\n[Frames processing]")
            folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')
            file_list = os.listdir(folder_path)

            for file_name in file_list:
                    if file_name.startswith('._'):
                        file_path = os.path.join(folder_path, file_name)
                        try:
                            os.remove(file_path)
                            print(f"\nRemoved unidentified image: {file_name}")
                        except OSError as e:
                            print(f"\nError removing image {file_name}: {e}")

            try:
                results = predictor.classify_useless_images(folder_path=folder_path, ckpt_path=jacques_model_path)
                results_of_all_sessions = pd.concat([results_of_all_sessions, results], axis=0, ignore_index=True)
            except Exception as e:
                print(f"\n[ERROR] Classification error in {directory}: {e}")
                pass
        
                    

    return results_of_all_sessions

def restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, thumbnails_creation):
    '''
    Function to restructure sessions folders to be "Zenodo ready"
    # Input:
        - sessions:
            ## the list of sessions:
                either a single directory that contains every sessions. (ex: '/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_unzipped/')
                or a list of sessions path (ex: ['/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_unzipped/session_2017_11_19_paddle_Black_Rocks'])
        - zip_sessions:
            ## either to zip the sessions or not
        - dest_path:
            ## destination path where useless images will me moved. (ex: '/home3/datawork/aboyer/mauritiusSessionsOutput/useless_images/')
        - annot_path:
            ## path where multilabel annotations csv will be created. (ex: '/home/datawork-iot-nos/Seatizen/seatizen_to_zenodo/mauritius_sessions_processing_output/results_csv/herbier_classification/herbier_classification_.csv')
        - jacques_model_path:
            ## path to jacques model. (ex: '/home/datawork-iot-nos/Seatizen/models/useless_classification/version_17/checkpoints/epoch=7-step=2056.ckpt')
        - annotation_model_path:
            ## path to annotation model. (ex: '/home/datawork-iot-nos/Seatizen/mauritius_use_case/Mauritius/models/multilabel_with_sable.pth')
        - thresholds_labels:
            ## a dictionnary that contains labels names and their associated thresholds
        - thumbnails_creation:
            ## either or not to create thumbnails of the images
    # Output:
        - sessions folders "Zenodo ready"
    '''
    if len(sessions) != 0:
        # classification
        if len(jacques_model_path) != 0:
            results_of_all_sessions = classify_sessions(sessions, session_index, jacques_model_path)
        else:
            results_of_all_sessions = pd.DataFrame(data={}) # empty dataframe
        
        list_of_sessions = get_sessions_list(sessions, session_index)
        
        # operations on each session
        for session in list_of_sessions:
            session_name = session.split('/')[-1]

            if not results_of_all_sessions.empty:
                # export results to csv file
                model = jacques_model_path.split('/')[-1]
                # adding session name, jacques version and classification model version to csv filename
                suffix = f"{session_name}_jacques-v0.1.0_model-{model.split('.')[0]}"
                class_path = os.path.join(session, 'LABEL/classification_.csv')

                if not os.path.exists(os.path.dirname(class_path)):
                    class_path = os.path.join(session, 'PROCESSED_DATA/IA/')
                    os.makedirs(class_path, exist_ok=True)
                    class_path = os.path.join(class_path, 'classification_.csv')

                class_path = class_path[:-4] + suffix + class_path[-4:]
                results_of_all_sessions.to_csv(class_path, index = False, header = True)
                print(f'\nClassification informations written at {class_path}\n')

                # create a .txt file in the METADATA folder of each session. This file give statistics about the jacques classification result.
                try:
                    print(f"Creation of {session_name} jacques stats file...")
                    create_sessions_stats(session, session_name, jacques_model_path)
                except Exception as e:
                    print(e)
            
            # move useless images
            if len(dest_path) != 0:
                dest_path = os.path.join(dest_path, session_name)
                move_out_images(class_path, dest_path, who_moves='useless')
            
            image_folder_path = f'{session}/DCIM/'
            if os.path.exists(image_folder_path):
                # delete BEFORE and AFTER folders if they exists
                delete_folders(image_folder_path)
            else:
                image_folder_path = os.path.join(session, 'PROCESSED_DATA/FRAMES/')

            
            for key, path in annot_path.items():
                if len(path) != 0:
                    print(f"\nStarting annotations using {key} model.")
                    csv_annotation_path = path[:-4] + session_name + path[-4:]
                    final_csv_path = os.path.join(os.path.dirname(csv_annotation_path), "final_annotation_.csv")
                    final_csv_path = final_csv_path[:-4] + session_name + final_csv_path[-4:]

                    try:
                        model_path = annotation_model_path.get(key)
                    except KeyError:
                        print(f"{key} not found in annotation_model_path definition in config.json file.")
                    try:
                        model_thresholds_labels = threshold_labels.get(key)
                    except KeyError:
                        print(f"{key} not found in threshold_labels definition in config.json file.")

                    # adding model annotations
                    
                    list_of_dir = get_subfolders(image_folder_path)
                    list_of_df = []

                    if len(list_of_dir) != 0:
                        for directory in list_of_dir:
                            print(f"\nAnnotations of images located in {directory}...")
                            images_path = os.path.join(session, directory)
                            df = annotation_model(images_path, csv_annotation_path, model_path, model_thresholds_labels)
                            list_of_df.append(df)

                        df = pd.concat(list_of_df, ignore_index=True)
                    else:
                        df = annotation_model(image_folder_path, csv_annotation_path, model_path, model_thresholds_labels)
                        
                    df.sort_values(by="image", inplace=True)
                    df.to_csv(csv_annotation_path, index = False, header = True)
                    print(f'\nAnnotations CSV of {session} successfully created at {csv_annotation_path}\n')
                    
                    # join GPS metadata to annotation file
                    gps_info_csv_path = f'{session}/GPS/photos_location_{session_name}.csv'
                    if os.path.exists(gps_info_csv_path):
                        join_GPS_metadata(csv_annotation_path, gps_info_csv_path, final_csv_path)
                        print(f'Merged GPS metadata with annotation results at {final_csv_path}\n')
                    else:
                        gps_info_csv_path = f'{session}/METADATA/metadata.csv'
                        join_GPS_metadata(csv_annotation_path, gps_info_csv_path, final_csv_path)

            # zip session folder if true in config file
            if len(zipped_sessions_path) != 0:
                print(f"Zipping {session_name}...")
                try:
                    copy_and_zip_folder(session, zipped_sessions_path, session_name)
                    print(f"\n{session_name} successfully zipped.")
                except Exception as e:
                    print(e)

            if thumbnails_creation:
                # creation of a folder with thumbnails of images located in DCIM or FRAMES
                create_thumbnails(session)
    else:
        print("[ERROR] You must fill in a path to your sessions in the config.json file! Refer to the README.md file for more informations.")
            
        
def main():
    start_time = time.time()
    print(f"Start time: {datetime.now()}")
    # getting session index from script args
    parser = argparse.ArgumentParser()
    parser.add_argument("--session-index",
                        action="store",
                        type=int,
                        default=argparse.SUPPRESS,
                        help="Index of the session that is being processed.")
    args = parser.parse_args()
    
    # read the config.json file
    with open('config.json') as json_file:
        config = json.load(json_file)
    
    # getting all variables from config.json file
    ## models
    jacques_model_path = config["paths"]["jacques_model_path"]
    annotation_model_path = config["paths"]["annotation_model_path"]
    ## paths
    sessions = config["paths"]["sessions_path"]
    zipped_sessions_path = config["paths"]["zipped_sessions_path"]
    dest_path = config["paths"]["useless_images_path"]
    annot_path = config["paths"]["annotation_csv_path"]
    ## threshold labels dictionnary
    threshold_labels = config["threshold_labels"]
    ## thumbnails creation
    thumbnails_creation = config["create_thumbnails"]
    
    if hasattr(args, "session_index"):
        session_index = args.session_index - 1
        restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, thumbnails_creation)
        execution_time_seconds = time.time() - start_time
        execution_time_minutes = "{:.2f}".format(execution_time_seconds / 60)
        print("\n========================================================")
        print(f"\nEnd time: {datetime.now()}")
        print(f"Total execution time: {execution_time_minutes} minutes")
    else: # execution on all sessions in the folder
        session_index = -1
        restructure_sessions(sessions, session_index, zipped_sessions_path, dest_path, annot_path, jacques_model_path, annotation_model_path, threshold_labels, thumbnails_creation)
        execution_time_seconds = time.time() - start_time
        execution_time_minutes = "{:.2f}".format(execution_time_seconds / 60)
        print("\n========================================================")
        print(f"\nEnd time: {datetime.now()}")
        print(f"Total execution time: {execution_time_minutes} minutes")

if __name__ == '__main__':
    main()
